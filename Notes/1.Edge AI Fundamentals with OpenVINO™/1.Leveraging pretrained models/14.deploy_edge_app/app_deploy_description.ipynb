{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Steps and Car Meta Model Output Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for calling preprocessing and utilizing the output handling functions from within app.py is fairly straightforward:\n",
    "\n",
    "`preprocessed_image = preprocessing(image, h, w)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just feeding in the input image, along with height and width of the network, which the given `inference_network.load_model` function actually returned for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`output_func = handle_output(args.t)\n",
    "processed_output = output_func(output, image.shape)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is partly based on the helper function I gave you, which can return the correct output handling function by feeding in the model type. The second line actually sends the output of inference and image shape to whichever output handling function is appropriate.\n",
    "\n",
    "#### Car Meta Output Handling\n",
    "Given that the two outputs for the Car Meta Model are `\"type\"` and `\"color\"`, and are just the softmax probabilities by class, I wanted you to just return the `np.argmax`, or the index where the highest probability was determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-6c50c2f18ec5>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-6c50c2f18ec5>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    color_pred = np.argmax(color)`\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def handle_car(output, input_shape):\n",
    "    '''\n",
    "    Handles the output of the Car Metadata model.\n",
    "    Returns two integers: the argmax of each softmax output.\n",
    "    The first is for color, and the second for type.\n",
    "    '''\n",
    "    # Get rid of unnecessary dimensions\n",
    "    color = output['color'].flatten()\n",
    "    car_type = output['type'].flatten()\n",
    "    # TODO 1: Get the argmax of the \"color\" output\n",
    "    color_pred = np.argmax(color)`\n",
    "    # TODO 2: Get the argmax of the \"type\" output\n",
    "    type_pred = np.argmax(car_type)\n",
    "    return color_pred, type_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Car Meta Model\n",
    "I have moved the models used in the exercise into a `models subdirectory` in the `/home/workspace directory`, so the path used can be a little bit shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python app.py -i \"images/blue-car.jpg\" -t \"CAR_META\" -m \"/home/workspace/models/vehicle-attributes-recognition-barrier-0039.xml\" -c \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the other models, make sure to update the `input image -i, model type -t, and model -m `accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation Output Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling the car output was fairly straightforward by using `np.argmax`, but the outputs for the pose estimation and text detection models is a bit trickier. However, there's a lot of similar code between the two. In this second part of the solution, I'll go into detail on the pose estimation model, and then we'll finish with a quick video on handling the output of the text detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pose Estimation is more difficult, and doesn't have as nicely named outputs. I noted you just need the second one in this exercise, called `'Mconv7_stage2_L2'`, which is just the keypoint heatmaps, and not the associations between these keypoints. From there, I `created an empty array to hold the output heatmaps once they are re-sized, as I decided to iterate through each heatmap 1 by 1 and re-size it, which can't be done in place on the original output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_pose(output, input_shape):\n",
    "    '''\n",
    "    Handles the output of the Pose Estimation model.\n",
    "    Returns ONLY the keypoint heatmaps, and not the Part Affinity Fields.\n",
    "    '''\n",
    "    # TODO 1: Extract only the second blob output (keypoint heatmaps)\n",
    "    heatmaps = output['Mconv7_stage2_L2']\n",
    "    # TODO 2: Resize the heatmap back to the size of the input\n",
    "    # Create an empty array to handle the output map\n",
    "    out_heatmap = np.zeros([heatmaps.shape[1], input_shape[0], input_shape[1]])\n",
    "    # Iterate through and re-size each heatmap\n",
    "    for h in range(len(heatmaps[0])):\n",
    "        out_heatmap[h] = cv2.resize(heatmaps[0][h], input_shape[0:2][::-1])\n",
    "\n",
    "    return out_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `input_shape[0:2][::-1] `line is `taking the original image shape of HxWxC`, taking just the first two (HxW), and reversing them to be WxH as `cv2.resize uses`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Detection Model Handling\n",
    "The code for the text detection model is pretty similar to the pose estimation one, so let's finish things off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Detection had a very similar output processing function, just using the `'model/segm_logits/add'` output and only needing to resize over two \"channels\" of output. I likely could have extracted this out into its own output handling function that both Pose Estimation and Text Detection could have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_text(output, input_shape):\n",
    "    '''\n",
    "    Handles the output of the Text Detection model.\n",
    "    Returns ONLY the text/no text classification of each pixel,\n",
    "        and not the linkage between pixels and their neighbors.\n",
    "    '''\n",
    "    # TODO 1: Extract only the first blob output (text/no text classification)\n",
    "    text_classes = output['model/segm_logits/add']\n",
    "    # TODO 2: Resize this output back to the size of the input\n",
    "    out_text = np.empty([text_classes.shape[1], input_shape[0], input_shape[1]])\n",
    "    for t in range(len(text_classes[0])):\n",
    "        out_text[t] = cv2.resize(text_classes[0][t], input_shape[0:2][::-1])\n",
    "\n",
    "    return out_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
