{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Benchmarking\n",
    "So far you have seen how to run a simple or baseline benchmark test on the workbench. In this section, we will see how you can perform many tests with various configurations of batch size and streams. In the end, we will be able to see which is the combination of batch size and stream for your model and hardware that gives the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, `you will have to run a range of performance benchmarks on a model of your choice. You will have to find the best combination of stream and batch size for your model.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are your tasks for this exercise:\n",
    "\n",
    "* Load model on to DL workbench\n",
    "* Load an autogenerated dataset\n",
    "* Run performence benchmark on range of stream and batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, DL Workbench can help us get accurate performance metrics about our model. `But there are a few points that you should keep in mind when interpreting the results you get:`\n",
    "\n",
    "* **DL Workbench only gives performance results of your model and not your application code**. There might be bottlenecks in your application code that could cause your system to perform poorly. We will see how to measure these bottlenecks in the next lesson.\n",
    "\n",
    "\n",
    "* **DL Workbench gives you performance results only for the hardware you select.** You might get different results for different hardware.\n",
    "\n",
    "\n",
    "* **Even though the model might give you the best performance at a certain value of batches and streams for a given hardware, your scenario may not generate data fast enough to fulfill those batch and stream requirements**. In a situation like this, your model will have to wait for data, which might offset the performance gains of batching and streaming. You should therefore select the batch and stream sizes that takes into account not only the throughput and latency, but also how data is generated in your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of advanced benchmarking\n",
    "#### * We can use it to measure the performance of our model on our hardware\n",
    "#### * We can use it to find the optimal number of batches and streams where our model performs optimally"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
