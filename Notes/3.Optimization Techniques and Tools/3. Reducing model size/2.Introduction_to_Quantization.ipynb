{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, quantization is the process of mapping values from a larger set to a smaller one. In quantization, we might start off with a continuous (and perhaps infinite) number of possible values, and map these to a smaller (finite) set of values. In other words, `quantization is the process of reducing large numbers of continuous values down into a limited number of discrete quantities.`\n",
    "\n",
    "In the present context, **`we can use quantization to map very large numbers of high-precision weight values to a lower number of lower-precision weight valuesâ€”thus reducing the size of the model by reducing the number of bits we use to store each weight`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural network can have many parameters or weight values.\n",
    "  * For instance an inception v3 model has more than 23 mn parameters.\n",
    "  * Usually these values are saved in high precision,like floats which can require upto 32 bit of space for each parameter.  \n",
    "  \n",
    "  * Saving all these parameters require space which can be limited in memory constrained edge computing system.\n",
    "  * Performing computations on high precision values requires more time and power.\n",
    "  \n",
    "* How to store large network on edge device while using less memory?\n",
    " * Use Quantization (mapping high precision values to low precisions)\n",
    "   * instaed of using 32 bit for each weight we can use 8 bit,reducing model size by one fourth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reduction in memory\n",
    "* `Uses floating point arithmetic` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is done **`only to reduce on device storage acquired not to reduce the flops or inference time`**\n",
    "* There is no reduction in inference time because quantized weights are converted to floats during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight and Activation quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Both weights and activations of the output from each layer are quantized ` \n",
    "* Primary goal of technique is reduce computaional complexity by using integer arithmetic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reduction in memory\n",
    "* **`Uses integer arithmetic`**\n",
    "* `Affine transformation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more difficult to do because operations being done on higher precision values need to match operations on lower precision values.\n",
    "This means mapping from high ones to low ones need to be affine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ternary Neural Network**\n",
    "  * three weights\n",
    "  * usually -1,0,1\n",
    "\n",
    "**Binary Neural Networks**\n",
    "  * two weights\n",
    "  * -1 and 1\n",
    "  \n",
    "* **`These networks are built to test the limits of model quantizatioon and compression`**\n",
    "* **`These networks also make use of bit shift operations on hardware to perform multiplication and other operations that also reduce computational power `**\n",
    "* OpenVINO  ships with some pre-converted versions of binary weighted models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`By quantizing both weights and activation in our network,we can improve the performance and optimize network for both memory size and floating point operations.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "affine transformation, which is a concept from linear algebra, you can read about the concept [here.](https://eli.thegreenplace.net/2018/affine-transformations/) `In this context, we use affine transformations because the ratio of the distances between the weights need to be the same before and after quantizing the model.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type of quantization to the effect that it has on the network\n",
    "\n",
    "|EFFECT|TYPE OF QUANTIZATION|\n",
    "|--------------|------------|\n",
    "|Reduction in computational complexity|Weight and Activation quantization|\n",
    "|Uses floating point numbers for operations|Weight Quantization|\n",
    "|Uses Integer Arithmetic|Weight and Activation Quantization|\n",
    "|Reduction in Memory|Both technique|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of unique weight values to the type of neural network\n",
    "|TYPE OF NEURAL NETWORK|NUMBER OF UNIQUE WEIGHTS|\n",
    "|-----------------------|-----------------------|\n",
    "|INT8 Quantized Neural Network|256|\n",
    "|Binary Neural Network|2|\n",
    "|Ternary Neural Network|3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Type of neural network to the number of bits you would need to store each weight\n",
    "|TYPE OF NEURAL NETWORK|NUMBER OF BITS|\n",
    "|----------------------|--------------|\n",
    "|INT8 Quantized Nerual network|8|\n",
    "|Binary Neural Network|1|\n",
    "|Ternary Neural Network|2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Learning\n",
    "Here's the paper we referred to in the video, in case you want to check it out:\n",
    "\n",
    "[XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks (Rastegari et al., 2016)](https://video.udacity-data.com/topher/2020/March/5e6e8859_xnor-net-imagenet-classification-using-binary-convolutional-neural-networks/xnor-net-imagenet-classification-using-binary-convolutional-neural-networks.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
