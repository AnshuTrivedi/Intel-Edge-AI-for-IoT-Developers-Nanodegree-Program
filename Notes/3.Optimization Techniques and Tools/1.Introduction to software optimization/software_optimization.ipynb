{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Software Optimization and Why Does it Matter?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Performance is a measure of how efficiently a system is performing its task. Typically, performance is measured in terms of the accuracy, inference speed, or energy efficiency of the system.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our efforts to improve the performance of a system, there are generally two approaches we can take. The first is hardware optimization:\n",
    "\n",
    "**`Hardware optimization`** \n",
    "* can be as simple as `changing to a new hardware platform` or \n",
    "* as complicated as `building a custom hardware `specifically designed to increase the performance of a particular application.\n",
    "\n",
    "And the second approach is software optimization:\n",
    "\n",
    "**`Software optimization`** involves `making changes to your code or model to improve your application's performance.` As applied to Edge computing, this will involve techniques and `algorithms that reduce the computational complexity of the models`.\n",
    "  * Use new model that takes 30 sec for inference than model that takes 35 sec  \n",
    "  * Refactor your image preprocessing code to reduce time in image preprocessing \n",
    "  * changing the model precision saved us time when loading our model.` Reducing the precision can also reduce the size of our model file.` So while our FP16 model does not give much improvement in terms of model loading time, it takes only about half the storage space of the FP32 model. `This would be advantageous if memory were a constraint in our system`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "One additional thing to keep in mind is that not all systems need to perform inference at the same rate. For example, as we noted in the video, a system that is performing inference on a video feed from a parking lot may be taking in a lower number of frames per second than one one that is running inference on high-speed traffic. This is something you should consider when looking into whether software optimization would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Software Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most deep learning applications, loading and performing inference on a model takes up the most time. For this reason, many of the techniques that have been developed focus on **model optimization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Unless otherwise specified, whenever we talk about software optimization in this course, we are referring to optimizing the **model and not the code**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, there are two ways to optimize our model, based on how the model is changed. We can:\n",
    "\n",
    "* **`Reduce the size of the model:`**\n",
    "This will reduce the time it takes to load the model and perform inference by removing unimportant or redundant parameters from our network.\n",
    "  * The model will load faster\n",
    "  * The model will compile faster\n",
    "  * Less space will be required for storing the model\n",
    "  * There will be a reduction in the number of model parameter\n",
    "\n",
    "**`Reduce the number of operations:`**\n",
    "This will reduce the time taken to perform inference by reducing the number of operations or calculations needed to execute the network. This can be done by `using more efficient layers and by removing connections in between neurons in our model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A larger \"teacher\" model trains a smaller model- **Knowledge distillation**\n",
    "\n",
    "* High precision weights are converted to low precision weights- **Quantization**\n",
    "\n",
    "* Model size is reduced by reducing the number of weights that need to be stored- **Model compression**\n",
    "\n",
    "* Neurons or connections between nerons are removed in the model- **Model pruning**\n",
    "\n",
    "* A computationally expensive layer is replaced with a computationally simple one- **Using efficient layers**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`A metric is a quantity or an attribute of a system that can be measured. A metric should help us infer useful information about a system.`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of an Edge AI system, we want to measure two kinds of performance:\n",
    "\n",
    "**Software Performance:**\n",
    "\n",
    "This is used to understand the properties of our model and application. Model accuracy is a good example of a metric used to measure software performance.\n",
    "\n",
    "**Hardware Performance:** \n",
    "\n",
    "This is used to understand the properties of the device our model is running on. For instance, power consumption is a hardware metric that can be used to decide the size of battery our system will require.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Energy table for 45nm CMOS process.\n",
    "\n",
    "|Operation\t|Energy [pJ]\t|Relative Cost|\n",
    "|-------|-------|-------|\n",
    "|32 bit int ADD\t|0.1\t|1|\n",
    "|32 bit float ADD\t|0.9\t|9|\n",
    "|32 bit Register File\t|1\t|10|\n",
    "|32 bit int MULT\t|3.1\t|31|\n",
    "|32 bit float MULT\t|3.7\t|37|\n",
    "|32 bit SRAM Cache\t|5\t|50|\n",
    "|32 bit DRAM Memory\t|640\t|6400|\n",
    "\n",
    "Adapted from Figure 1 of [ Learning both Weights and Connections for Efficient Neural Networks (Han et al., 2015).](https://arxiv.org/pdf/1506.02626.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION \n",
    "\n",
    "**From the table, what can you conclude about the energy taken to access memory vs. the energy taken to perform operations?**\n",
    "\n",
    "answer:`The energy taken to access memory is far greater than the energy taken to perform operations`\n",
    "\n",
    "\n",
    "`This is why we need to reduce the size of our model. A model with either fewer weights or smaller weight sizes (for instance 8 bit weights instead of 32 bits) will consume much lesser energy.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Metrics**\n",
    "* **Inference time**\n",
    "  * should be reduced to increase performance\n",
    "* **Model size**\n",
    "  * smaller models takes less time and energy to load\n",
    "* **Accuracy**\n",
    "  * If there is no regard for power and cost,deployment environment is not resource constrained , highly accurate model is       deployed even if model becomes complicated and inference time high.\n",
    "  * should be high but not at cost of other metriccs\n",
    "* **Precision and recall**  (in classification and recommendation)\n",
    "  \n",
    "**Recall**\n",
    "ratio of true positive to all instances\n",
    "\n",
    "**Precision***\n",
    "ratio of true positive to all true positives\n",
    "\n",
    "* **Latency and throughput**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latency and Throughput\n",
    "Latency and throughput are closely related metrics but not the same thing.\n",
    "\n",
    "**`Latency is the time taken from when an image is available for inference until the result for that image is produced`**.\n",
    "  * Latency is the time it takes to generate output for a given input.\n",
    "  * Latency is measured either in seconds or milliseconds\n",
    " \n",
    "\n",
    "\n",
    "**`Throughput is the amount of data that is processed in a given interval of time.`**\n",
    "  * Throughput is number of input that can be processed in a given particular time.\n",
    "  * Throughput is measured in seconds or minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Low latency** |**High throughput**|\n",
    "|-----|-----------|\n",
    "|One image at a time|five images at a time|\n",
    "|inference time: 1 sec|bacth inference time: 2 sec|\n",
    "|latency: 1 sec|latency:2 sec|\n",
    "|predictions/min: Latency X time >>  1 X 60|predictions/min: predictions X time >> 5 X (60/2)|\n",
    "|Throughput : 60 frames per minute(1fps)|Throughput: 150 frames per min(2.5 fps)|\n",
    "| Latency=1/Throughput|Latency=Batchsize/Throughput|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choosing higher throughput or low latency depends on your application\n",
    "* High throughput is used to generate results for large volume of data and more useful when you need to analyse more amount of data in given time.\n",
    "* For instance if you have single edge device analysing security camera footage from multiple cameras low latency will not be as important as high throughput\n",
    "* If edge device is controling the steering wheel of self driving car, then processing data sequencially at much faster rate is more imp than processing large amount of data.\n",
    "* **Lower latency is more important for real time applications**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Latency|Throughput|\n",
    "|-------|-----|\n",
    "|For quickly processing single data points|For processing large volume of data|\n",
    "|Inference is performed as soon as data is available and compute resources are free|Data is strored until batch is formed,then inference is performed|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Other Performance Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**System power**\n",
    "* optimize for longer operating time(in unreliable power source area)\n",
    "\n",
    "**System size**\n",
    "* Optimize for less volume\n",
    "   * FPGA will give much better latency and throughput but occupy more space and power to run\n",
    "   \n",
    "**System cost**\n",
    "* optimize for low deployment cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FLOP and MAC**\n",
    "\n",
    "* **FLOP**  floating point operations\n",
    "  * operations that involve floating point values\n",
    "  * Any operation like multiplication or addition that involves floating point value are called FLops.\n",
    "  * if weights are stored as float values in neural network then running inference on that model requires to do FLOPs.\n",
    "  * performing FLOPs require energy and time,measuring no of flops in your network,you can estimate time it will take to execute    your model \n",
    "  * more no of flops ,longer time to execute\n",
    "  \n",
    " \n",
    " \n",
    "* **MAC** Multiply And Accumulate\n",
    "* Multiply followed by addition\n",
    "* MAC operations are typical in Neural Network where widths and actiavtions are first multiplied  then added with other width and activation product.\n",
    "* performing MAC operation involves two flops:add and multiply\n",
    "* 1 MAC =2 FLOPs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FLOPs**\n",
    "\n",
    "* One way to measure inference time for your model is `to calculate the total number of computations the model will have to perform`.\n",
    "* And a common metric for measuring the number of computations is the FLOP.\n",
    "* `Any operation on a float value is called a Floating Point Operation or FLOP`.\n",
    "* This could be an addition, subtraction, division, multiplication, or—again—any operation that involves a floating point value. * By calculating the total FLOPs (Floating Point Operations) in a model, `we can get a rough estimate of how computationally complex our model is and how long it might take to run an inference on that model.`\n",
    "* The more FLOPs or Floating Point Operations a device can perform in a given time, the more computationally powerful that device will be.\n",
    "* Hardware devices are generally rated for the number of Floating Point Operations they can perform in a second. This is known as FLOPS or Floating Point Operations per Second.\n",
    "* Note that FLOPs (lowercase \"s\") is different than FLOPS (capital \"S\"). FLOPs is a quantity and represents the total number of floating point operations that a device needs to perform, whereas `FLOPS is a rate, and can tell us how many FLOPs a device can perform in a second.`\n",
    "\n",
    "**MACs**\n",
    "* Computations in neural networks typically involve a multiplication and then an addition. For instance, in a dense layer, we multiply the activation of a neuron with the weight for that neuron connection and then add it to another similar product:\n",
    "\n",
    "$Layer_{output}$ = $W_{1}*A_{1}+W_{2}*A_{2}+...+W_{n}*A_{n}$\n",
    "\n",
    "* Since these operations involve performing a multiplication and then an addition, they are called Multiply-Accumulate operations or simply MACs.\n",
    "* Since each MAC involves two operations (a multiplication and an addition), this means that we can generally think of one MAC as involving two FLOPs:\n",
    "* 1 MAC ~ 2 FLOPs\n",
    "* Actually, in some hardware (especially hardware that is optimized for running many MACs), the multiplication and addition are fused into a single operation. But for the purposes of this course, we'll assume the above (1 MAC = 2 FLOPs) is true when performing calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|OPTIMIZATION  |     METRIC|\n",
    "|--------------|------------|\n",
    "|A battery-powered edge system was using Bluetooth Low Energy for sharing log data. This was then removed as there was a wired interface that could do the same.\n",
    "This increased in the system on time.|system power|\n",
    "|In an edge system, a larger USB-A interface was replaced with a smaller micro-USB interface.|system size|\n",
    "|An Atom processor was replaced with multiple Neural Compute Sticks in an Edge Computing system|system cost|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`When you change one metric this will usually affect another metric, and often not in a good way. As a developer trying to optimize your model for the edge, you need to be wary of the side-effects of any changes you're making. The paper,` [An Analysis of Deep Neural Network Models for Practical Applications](https://arxiv.org/pdf/1605.07678.pdf) `compares and analyzes the relationships between the metrics we discussed and gives detailed information that can help you design efficient neural networks for the edge.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When do we do Software Optimization?\n",
    "Before you actually do software optimization, `it is important to know when it will not give you the results you are looking for.`\n",
    "\n",
    "\n",
    "* One of the easiest ways to check whether optimizing the model will help is `to compare the time it takes to perform inference,` and perform the `other bottlenecks` in our application code `like pre-processing`. \n",
    "* If the pre-processing time is significantly more than the model inference time then optimizing the model will not give much performance improvements. \n",
    "\n",
    "If you want to try it for yourself, you can download the code used in the video [here](https://video.udacity-data.com/topher/2020/March/5e7b6c2c_profiling/profiling.py) or from the link at the end of this page.\n",
    "\n",
    "Note: In the video, we will be using a line profiler. `A line profiler tells us the time it takes to run each line of code.` In particular we will be using [this](https://github.com/rkern/line_profiler) line profiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario-Based Software Optimization\n",
    "\n",
    "* Remember that the main purpose of software optimization is to reduce the inference time of our model when we are executing our model on a device with limited computational resources. However, not every system needs to perform at the same rate, so whether you need to perform software optimization will depend a lot on the specific scenario.\n",
    "\n",
    "* For instance, as we called out earlier in this lesson, if you are trying to read license plate numbers at a parking ticket kiosk, you don't need to have a system that performs inference every few milliseconds. Since cars generally stop for a few seconds at these kiosks, even if your system runs inference every second or two, your system will still function properly.\n",
    "\n",
    "* On the other hand, if you were trying to read license plate numbers on a busy highway, your system would need to perform inference on frames very quickly—so in this scenario, software optimization would be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the topics we covered in this lesson:\n",
    "\n",
    "* What is Software Optimization?\n",
    "* Why do we Need Software Optimization?\n",
    "* Types of Software Optimization\n",
    "* When to use Optimization Techniques\n",
    "* Metrics to Measure Performance\n",
    "* Other Metrics\n",
    "  * Power\n",
    "  * Cost\n",
    "* When do we do Software Optimization?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
